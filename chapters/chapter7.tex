\chapter{Thesis discussion}\label{chap:chapter7}

In this Thesis, I have developed and built upon statistical reliability methods using the Bayesian framework to make these models appliable to maintenance problems in the mining and mineral processing domain. The works in the two main body parts of the Thesis were motivated by industry problems---found through my time on industry placements---that were not solvable by using methods from the reliability literature due to the incompleteness and noisiness of the industry data. Through the two parts, I address two main \textit{theory-practice gaps} that will support reliability engineers in domains such as mining and mineral processing industry. The first, addressed in Part~\ref{part:one}, is handling partially observed lifetime data that can be both left-truncated with unknown exposure history and right-censored, a case that naturally arises when units have been repeatedly replaced for many years and only the failure records after a given date are available. Part\ref{part:two} looked at the gamma process and how, through the Bayesian hierarchical modelling formalism, necessary, non-trivial extensions can be incorporated: such as the need to account for measurement error or unit-to-unit variability, and how to accommodate wearing surfaces.

In these final pages of the Thesis, I start in Sec.~\ref{sec:thesis-summary} by summarising the work from the body of the Thesis and emphasise the key contributions. Section~\ref{sec:Thesis-future-work} then revisits the most significant areas for future work. Finally, In Sec.~\ref{sec:Thesis-practical}, I discuss the significance of the work for reliability practitioners in industry and conclude the Thesis.

\section{Overview and main discussions} \label{sec:thesis-summary}

Part~\ref{part:one} of the Thesis---consisting of Chaps.~\ref{chap:chapter2} and ~\ref{chap:chapter3}---focused on lifetime analysis, specifically of lifetime data that can be both left-truncated with unknown exposure history and right-censored. In Chap.~\ref{chap:chapter2}, I developed a Bayesian model for such lifetime data that imputes the true values of the partially observed lifetimes and, along with them, the missing truncation times. I also develop the method originally proposed by \citet{kaminskiy2005} for eliciting a joint prior for the Weibull parameters by embedding it fully within a Bayesian lifetime model so that the prior is properly filtered through the likelihood of the observed data. Using simulated data, I demonstrated the imputation method with both a weakly informative prior and an informative joint prior, constructed using the developments on the method of \citet{kaminskiy2005}. I compare the imputation method with the alternative option when exposure histories are unknown, which is to discard the left truncated lifetimes, and the ideal case where the exposure histories for the left-truncated lifetimes are known. Chapter~\ref{chap:chapter2} ended with a small simulation study to evaluate the conditions under which it is suitable to use the imputation treatment of the left-truncated lifetimes with unknown exposure history. I conclude that the imputation method is appropriate unless a) the period of observation is much shorter than the a priori mean lifetime or b) the time of the beginning of the observation is close to the beginning of the renewal process. In these two situations, the alternative is to discard the left-truncated lifetimes, in which case, a well constructed informative joint prior, constructed using the proposed method from is Sec.~\ref{sec:weibull-joint-prior}, can be used to account for the lost information about longer lifetimes.

Left truncation with unknown exposure time is difficult to identify in a dataset and is easily confused with simple censoring. It is important to either account for the truncation in the lifetimes that begin before the beginning of the observation period or to discard the lifetimes completely; otherwise, the results of the analysis will be biased. However, when formulating the likelihood of the data given the parameters in the usual way (that is, integrating out the censored observation), there is no obvious way of accounting for the missing truncation times. I have shown that if we instead take the approach of imputing the partially observed censored lifetimes, any lifetimes that begin before the observation period are a case of censoring, and by imputing their values, the corresponding missing truncation time can also be calculated. I've also shown that this is easily implemented in probabilistic programming languages such as Stan.

The second main body chapter of Part~\ref{part:one}, Chap.~\ref{chap:chapter3}, applied the methods developed in Chap.~\ref{chap:chapter2} to the industry dataset of idler-frame lifetimes. In the chapter, I analysed the idler-frame lifetimes with an informative prior, imputing the partially observed samples and truncation times to perform inference and retain as much of the information in the dataset as possible. I then showed how, by imputing the partially observed lifetimes in the model, the posterior contains predictive draws for the frames still under test conditioned on their age at the end of observation. Using these draws, I showed how to generate predictive distributions for the RUL of frames still in operation at the end of the observation period and for the cumulative number of failures going forward from the end of observation. Lastly, in the chapter, I showed how the joint draws of the Weibull parameters can be propagated through a cost function to incorporate uncertainty from the analysis when choosing a fixed time replacement interval for the idlers on the conveyor. 

Part~\ref{part:two} of the Thesis focused on degradation modelling, mainly using gamma stochastic processes. The first two chapters, Chaps.~\ref{chap:chapter4} and~\ref{chap:chapter5}, focused on theoretical developments of the gamma process model to include noisy observations and unit-to-unit variability. The third and final chapter, Chap.~\ref{chap:chapter6}, applied the gamma process to the industry dataset of conveyor belt wear measurements and made comparisons with a linear general path model.

Chapter~\ref{chap:chapter4} organised some of the literature on the noisy gamma process by showing how the Bayesian hierarchical formalism allows us to frame a model for a noisy gamma stochastic process in a tractable and transparent manner. Decomposing the noisy degradation model into data, process, and parameter models removes the need for complex deconvolution that requires the evaluation of, or approximations to, multidimensional integrals. The chapter also presented a reparametrisation of the gamma process in terms of the mean $\mu$ and coefficient of variation $\nu$, which simplifies prior specification since $\mu$ is clearly interpretable as the average wear rate and $\nu$ is the volatility of the process. In addition, the parameters $\mu$ and $\nu$ are orthogonal, which has desirable computational benefits.

I demonstrated the hierarchical noisy gamma process with parameters $\mu$ and $\nu$ on a simulated dataset and, in doing so, highlighted a presymptomatic non-identifiability between the scale of the measurement error and the volatility of the gamma process when the model is fit to a small number of noisy observations. This observation is particularly important in reliability applications since degradation datasets are often small. The weak identifiability between the standard deviation of the measurement error and the coefficient of variation of the gamma process highlights the importance of incorporating domain knowledge or supplementary data into the model or the partial pooling of information between similar units.

Chapter~\ref{chap:chapter5} then showed how the same BHM for the noisy gamma process can be naturally extended to include unit-to-unit variability in multiple noisily-observed degradation signals and how allowing some parameters to vary determines how information is shared between observational units. The proposed parameterisation in terms of $\mu$ and $\nu$ makes expansions of the model, such as unit-to-unit variability, more obvious, i.e. how similar do we expect the units to be in terms of their wear rate, their volatility, or both? In Chap.~\ref{chap:chapter5}, I fitted several variations of the noisy gamma process with unit-to-unit variability to experimental crack growth data with added noise. I demonstrated how to check and compare the models in a fully Bayesian framework visually inspecting the posterior predictive distributions and using elppd and cross-validation. In the last part of the chapter, I showed how to construct failure time distributions, with uncertainty, for units that are under test but have not failed and for new, unobserved units.

In Chap.~\ref{chap:chapter6}, I analysed the wear profile dataset from an iron ore conveyor's belt. To model the profiles, I proposed a functional data analysis approach, where the smooth B-spline functions that describe the wear profile at each observation time evolve according to a degradation model. I compared, side-by-side, a noisy gamma process and a linear general path model for the evolution of the spline coefficients. To the best of my knowledge, there is no work on functional degradation models in the literature. Using the functional models, I showed how to forecast the degradation profile of the belt into the future and how to construct failure time distributions that account for the uncertainty in the wear profile along the length of the belt.

In all of these chapters, I follow the precepts of good practice in the Bayesian statistical workflow and demonstrate: carefully constructing informative priors and checking these assumptions through prior predictive checking; assessing models using simulation; checking computation with HMC diagnostics; visualising the posterior through posterior predictive distributions or checking; and comparing models in a fully Bayesian framework. I place a particularly strong emphasis on how to encode prior information since either small sample sizes or weakly informative data (i.e. due to partially observed data or signals obscured by noise) are prominent problems in reliability datasets from the mining industry.

\section{Future directions} \label{sec:Thesis-future-work}

\paragraph*{Part~\ref{part:one}} The method I propose for imputing the partially observed left-truncated lifetimes and their truncation times needs further investigation. For example, I did not fully separate the two sources of potential problems with my approach: those arising due to implementation from those arising from breaches in assumptions. When implemented in Stan, there appears to be some updating in the imputed truncation times when the lifetime is both left-truncated by the beginning of the observation period and right-censored by its end, even though there should not be any information in the data to inform this parameter. The method could be implemented in Bugs \citep{lunn2012}, where it is possible to include nuisance parameters (using the cut function) that are not updated in the MCMC routine to overcome this issue, or the same could be achieved by ones own MCMC. However, for the case of the idler-frames, only one lifetime was both left truncated and right censored, and so this limitation of the method should not affect the results. There are also issues with the approach when the start of the observation period is too close to the origin of the repeat replacement process ($t = 0$). However, the boundary for what is acceptable is not clearly defined. More rigorous simulation would be necessary to identify this boundary.

The approach I have proposed and applied to the idler frame data assumes that all units (frames) are identical. However, additional information about the idler frames could be included in the model, such as where along the conveyor they are located---in the impact zone, at the head or tail pulley transitions---or the manufacturer and design---which is available for recently installed idlers. It would be both interesting and useful to allow for similar but different groups of idlers to share information, such as the Hierarchical models in Part~\ref{part:two} of the Thesis, and to include covariates in the model. Additionally, in the analysis of the idler-frames, there are a number of very short lifetimes that I treat as right-censoring events. These early failures still threaten to cause downtime. The infant mortalities could be included in a mixture model such as in \citet{mittman2013}; however, it would be important to understand how a mixture of lifetime distributions would affect the imputation of the partially observed left-truncated lifetimes and their truncation times.

\paragraph*{Part~\ref{part:two}} In fitting the noisy gamma process, there is a point as the scale of the noise increases when the noisy observation of the signal obscures the `jumpiness' of the underlying degradation trace, and it is much easier to fit a linear model rather than a stochastic process. More investigation is needed to understand the interplay between the volatility of a degradation signal and its noisy observation. In addition, noisy observations also obscure whether or not unit-to-unit variability is necessary in the model. For example, if we identify that unit-to-unit variability is present for a non-noisy set of degradation traces, is there a level of measurement error where the conclusion is not the same? A further question is how sensitive metrics such as $\mbox{elppd}_{\text{CV}}$ are in these cases, and would they lead us to the correct model in these circumstances? These questions would need to be explored through simulation, simulating data from a known model and seeing if the model can be identified under different noisy conditions.

In Chapter~\ref{chap:chapter6}, I have shown how FDA can be used to model a one-dimensional degrading surface. There are now companies that perform point cloud scans of a surface. Extending the method to two dimensions would be useful for this type of data. I also highlight in section~\ref{sec:belt-wear-discussion} that the model I propose does not account for large-scale spatial dependence between the spline coefficients. When analysing a two-dimensional dataset with observations on a densely observed grid, it would be informative to incorporate large-scale spatial effects into the model. One possible way of doing this is to use a conditional autoregressive (CAR) prior.

\section{Implications for industry practitioners} \label{sec:Thesis-practical}

Throughout this Thesis, the Bayesian approaches I have presented address real problems in reliability data from the mining and mineral processing sector: retaining the information from lifetimes that are left truncated with unknown exposure history, modelling multiple noisy degradation signals with gamma process models, and modelling the degradation of a wearing surface using functional data analysis. In addition to providing solutions to these practical problems, the approaches are implemented in Stan; an accessible and open-source probabilistic programming language, so that the models can be implemented and adjusted easily by a wide audience.

Throughout the work, I stress the components of good Bayesian statistical workflow and demonstrated their application in the reliability domain. These include thinking carefully about how a model can be encoded in order to simplify its implementation; evaluating and refining the model and priors through simulation before fitting the data; interpreting uncertainty in the posterior through visualisation and posterior predictive distributions; propagating this posterior uncertainty through useful utility functions such as cost functions or the failure time distribution; and comparing sets of Bayesian models in a fully Bayesian framework using elppd and cross-validation methods. In the interests of reproducible research, I provide all of the code to implement the models and perform the analysis of this Thesis on a Github repository [Leadbetter et al., 2024] so that other reliability practitioners may follow along, reproduce and apply my work in practice, and adapt the models and workflow to their own industry problems.

The most relevant components of this Bayesian statistical workflow to reliability applications, and therefore the ones I have focused on most deeply in this Thesis, are 1) the careful construction of the model and prior so that the analyst can take advantage of as much information in the data and as much supplemental information as possible, and 2) how he/she can incorporate the posterior uncertainty of the Bayesian into the subsequent maintenance decisions that the analysis aims to inform. 
Much of the historical reliability data in mining is small, messy, and often noisy, and as a result the analyst cannot afford to throw away data and needs to take full advantage of as much of the information as he/she has available to them, in both the data set being analysed, historic data and domain expert knowledge.
\begin{itemize}
  \item Reframing the likelihood for right censored and left truncated data in terms of missing data imputation can avoid having to discard left-truncated observations when their exposure history is unknown.
  \item Modelling multiple units simultaneously shares information to help identify the degradation model.
\end{itemize} 
Prior construction
\begin{itemize}
  \item A joint prior can be constructed for the two Weibull parameters so that information can be encoded in different parts of the lifetime distribution and how this is useful when data are partially observed.
  \item The gamma process can be re-parameterised in terms of $\mu$ and $\nu$ and how this makes clear what information is being encoded in the prior.
  \item How the informative prior for the mean wear rate of a GP or LM can be derived from the historic datasets in the belt wear case study.
\end{itemize}
Data in the mining and mineral processing Much of the historical datasets in mining are small and messy, and the wealth of industry domain expert knowledge is incredibly helpful in overcoming these limitations in the data.





I've also stressed the demonstration of simple examples of using the posterior of a Bayesian model to inform short and long-term maintenance decisions. Little in the literature guides practitioners on how to incorporate the uncertainty encoded in a posterior distribution into maintenance decisions. Even though this last step is paramount in the methods from the literature resulting in quantifiable impact in real applications. Here I have provided concrete examples of\dots
\begin{itemize}
  \item Derive RUL and cumulative failures for short term decisions in the idler frame case study.
  \item How to produce FT distributions for units under test and new units in the noisy gamma process with unit-to-unit variability.
  \item How to produce the FT distribution for the belt in the conveyor belt wear case study when the first passage time also depends on the variability along the length of the belt (the noisy level in the process model.)
\end{itemize}
More works in Bayesian reliability, like this one, should present examples of this last step in a Bayesian \textit{reliability} workflow.

\chapter{Discussion}\label{chap:chapter7}

In this thesis, I have developed and built upon statistical reliability methods using the Bayesian framework to make these models applicable to maintenance problems in mining. The works in this thesis were motivated by industry problems that I identified during my time on industry placements. They could not be addressed by using existing methods from the reliability literature due to the incompleteness and noisiness of the industry data. In the thesis, I addressed two main \textit{theory-practice gaps} whose solutions will support reliability engineers in domains such as the mining and mineral processing industry. The first, addressed in Part~\ref{part:one}, is handling partially observed lifetime data that can be both left-truncated with unknown exposure history and right-censored, a case that naturally arises when units have been repeatedly replaced for many years and only the failure records after a given date are available. Part~\ref{part:two} focussed on the gamma process and presented how, through the Bayesian hierarchical modelling formalism, necessary, non-trivial extensions can be incorporated: such as the need to account for measurement error or unit-to-unit variability, and how to accommodate wearing surfaces.

In this concluding chapter, Sec.~\ref{sec:thesis-summary} summarises the work in the thesis and outlines the key contributions. Section~\ref{sec:thesis-future-work} then stresses the most significant areas for future work and how they could be addressed. Finally, in Sec.~\ref{sec:thesis-practical}, I discuss the significance of the work for reliability practitioners in industry.

\section{Overview and main discussions} \label{sec:thesis-summary}

Part~\ref{part:one} of the thesis, consisting of Chaps.~\ref{chap:chapter2} and~\ref{chap:chapter3}, focussed on lifetime analysis, specifically of lifetime data that can be both left-truncated with unknown exposure history and right-censored. In Chap.~\ref{chap:chapter2}, I developed a Bayesian model for such lifetime data that imputes the true values of the partially observed lifetimes and, along with them, the missing truncation times. I also further develop the method originally proposed by \citet{kaminskiy2005} for eliciting a joint prior for the Weibull parameters by embedding it fully within a Bayesian lifetime model so that the prior is properly filtered through the likelihood of the observed data. Using simulated data, I demonstrated the imputation method with both a weakly informative prior and an informative joint prior, constructed using the extensions of the method of \citet{kaminskiy2005}. I compared the imputation method with the ideal case where the exposure histories for the left-truncated lifetimes are known and the alternative option when exposure histories are unknown, which is to discard the left-truncated lifetimes. Chapter~\ref{chap:chapter2} ended with a small simulation study to evaluate the conditions under which it is suitable to use the imputation treatment of the left-truncated lifetimes with unknown exposure history. I conclude that the imputation method is appropriate unless a) the period of observation is much shorter than the \textit{a priori} mean lifetime or b) the time of the beginning of the observation is close to the beginning of the renewal process. In these two situations, the alternative is to discard the left-truncated lifetimes, in which case, a well constructed informative joint prior, constructed using the proposed method from Sec.~\ref{sec:weibull-joint-prior}, can be used to account for the lost information about longer lifetimes.

Left-truncation with unknown exposure time is difficult to identify in a dataset and is easily confused with typical censoring. It is important to either account for the truncation in the lifetimes that begin before the start of the observation period or to discard the lifetimes completely; otherwise, the parameter estimates will be biased. However, when formulating the likelihood of the data in a way that integrates out the censored observation, there is no obvious way of accounting for the missing truncation times. I have shown that if we instead take the approach of imputing the partially observed censored lifetimes, any lifetimes that begin before the observation period can be viewed as censored, and by imputing their values, the corresponding missing truncation time can also be calculated. I have also shown how this is implemented in probabilistic programming languages such as Stan.

In Chap.~\ref{chap:chapter3}, I applied the methods developed in Chap.~\ref{chap:chapter2} to the dataset of idler frame lifetimes. In the chapter, I analysed the idler frame lifetimes with an informative prior, imputing the partially observed samples and truncation times to perform inference and retain as much of the information in the dataset as possible. I then showed how, by imputing the partially observed lifetimes in the model, the posterior contains predictive draws for the frames still under test conditioned on their age at the end of observation. Using these draws, I showed how to generate predictive distributions for the RUL of frames still in operation and for the cumulative number of failures from the end of observation. Lastly, in the chapter, I showed how the joint draws of the Weibull parameters can be propagated through a cost function to incorporate uncertainty from the analysis when choosing a fixed time replacement interval for the idlers on the conveyor. 

Part~\ref{part:two} of the thesis focussed on degradation modelling, mainly using gamma stochastic processes. Chapters~\ref{chap:chapter4} and~\ref{chap:chapter5} focussed on reframing of the gamma process model to include noisy observations and unit-to-unit variability that provides methodology for industry data. The third and final chapter, Chap.~\ref{chap:chapter6}, applied this methodology to the industry dataset of conveyor belt wear measurements and made comparisons with a linear general path model.

Chapter~\ref{chap:chapter4} provides a simpler framework for the noisy gamma process by showing how the Bayesian hierarchical formalism allows us to frame a model for a noisy gamma stochastic process in a tractable and transparent manner. Decomposing the noisy degradation model into data, process, and parameter models removes the need for complex deconvolution that requires the evaluation of, or approximations to, multidimensional integrals. The chapter also presented a reparametrisation of the gamma process in terms of the mean $\mu$ and coefficient of variation $\nu$, which simplifies prior specification since $\mu$ is clearly interpretable as the average wear rate and $\nu$ is the volatility of the process. In addition, the parameters $\mu$ and $\nu$ are orthogonal, which has desirable computational benefits.

I demonstrated fitting the hierarchical noisy gamma process with parameters $\mu$ and $\nu$ on a simulated dataset and, in doing so, highlighted a pre-asymptotic non-identifiability between the scale of the measurement error and the volatility of the gamma process when the model is fitted to a small number of noisy observations. This observation is particularly important in reliability applications since degradation datasets are often small. The weak identifiability between the standard deviation of the measurement error and the coefficient of variation of the gamma process highlights the importance of incorporating domain knowledge or supplementary data into the model or the partial pooling of information between similar units.

Chapter~\ref{chap:chapter5} then showed how the same BHM for the noisy gamma process can be naturally extended to include unit-to-unit variability in multiple noisily observed degradation signals and how allowing some parameters to vary determines how information is shared between observational units. The parameterisation in terms of $\mu$ and $\nu$ makes extensions of the model, such as modelling unit-to-unit variability, more obvious, i.e. how similar do we expect the units to be in terms of their wear rate, their volatility, or both? In Chap.~\ref{chap:chapter5}, I fitted several variations of the noisy gamma process with unit-to-unit variability to experimental crack growth data with added noise. I demonstrated how to check and compare the models in a fully Bayesian framework by visually inspecting the posterior predictive distributions and using elppd and cross-validation. In the last part of the chapter, I showed how to construct failure time distributions, with uncertainty, for units that are under test but have not failed and for new, unobserved units.

In Chap.~\ref{chap:chapter6}, I analysed the wear profile dataset from an iron ore conveyor's belt. To model the profiles, I proposed a functional data analysis approach, where the smooth B-spline functions that describe the wear profile at each observation time evolve according to a degradation model. To the best of my knowledge, there is no work on functional degradation models in the literature. I compared a noisy gamma process and a linear general path model for the evolution of the spline coefficients. Using the functional models, I showed how to forecast the degradation profile of the belt and how to construct failure time distributions that account for the uncertainty in the wear profile along the length of the belt.

%In all of these chapters, I follow the precepts of good practice in the Bayesian statistical workflow and demonstrate: carefully constructing informative priors and checking these assumptions through prior predictive checking; assessing models using simulation; checking computation with HMC diagnostics; visualising the posterior through posterior predictive distributions or checking; and comparing models in a fully Bayesian framework. I place a particularly strong emphasis on how to encode prior information since either small sample sizes or weakly informative data (i.e. due to partially observed data or signals obscured by noise) are prominent problems in reliability datasets from the mining industry.

\section{Future directions} \label{sec:thesis-future-work}

\paragraph*{Part~\ref{part:one}} The method I propose for imputing the partially observed left-truncated lifetimes and their truncation times needs further investigation. For example, I did not fully separate the two sources of potential problems with my approach: those arising due to implementation and those arising from violations of assumptions. For the implementation in Stan, there appears to be some updating in the imputed truncation times when the lifetime is both left-truncated by the beginning of the observation period and right-censored by its end, even though there should not be any information in the data to inform this parameter. The method could be implemented in BUGS \citep{lunn2012}, where it is possible to include nuisance parameters that are not updated in the MCMC routine to overcome this limitation. The same result could be achieved by writing a bespoke MCMC routine. However, for the case of the idler-frames, only one lifetime was both left-truncated and right-censored, and so this limitation of the method should not affect the results. With respect to violations of assumptions, there are also problems with the approach when the start of the observation period is too close to the origin of the repeat replacement process ($t = 0$). However, the boundary for what is acceptable is not clearly defined. More rigorous simulation would be necessary to identify this boundary.

The approach I have proposed and applied to the idler frame data assumes that all units (frames) are identical. However, additional information about the idler frames could be included in the model, such as where along the conveyor they are located---in the impact zone, at the head or tail pulley transitions---or the manufacturer and design, which is available for recently installed idlers. It would be both interesting and useful to allow for similar but different groups of idlers to share information, such as in the hierarchical models in Part~\ref{part:two}, and to include covariates in the model. Additionally, in the analysis of the idler-frames, there are a number of very short lifetimes that I treat as right-censoring events. These early failures still threaten to cause downtime. The infant mortalities could be included in a mixture model such as in \citet{mittman2013}; however, it would be important to understand how a mixture of lifetime distributions would affect the imputation of the partially observed left-truncated lifetimes and their truncation times.

\paragraph*{Part~\ref{part:two}} In fitting the noisy gamma process, there is a point at which the noisy observation of the signal obscures the `jumpiness' of the underlying degradation trace. At this point it is much easier to fit a linear model rather than a stochastic process. More investigation is needed to understand the interplay between the volatility of a degradation signal and the noise that contaminates it. In addition, noisy observations also obscure whether or not unit-to-unit variability is necessary in the model. For example, if we identify that unit-to-unit variability is present for a non-noisy set of degradation traces, is there a level of measurement error where the conclusion is not the same? A further question is how sensitive metrics such as $\mbox{elppd}_{\text{CV}}$ are in these cases, and would they lead us to the correct model in these circumstances? These questions would need to be explored through simulation, simulating data from a known model and seeing if the model can be identified under different noisy conditions.

In Chapter~\ref{chap:chapter6}, I have shown how FDA can be used to model a one-dimensional degrading surface. More recently, it has become possible to commission point cloud scans of the wearing surfaces on some assets at the mine site (although, not conveyors), which produces wear data in two dimensions across a fine grid. Extending the proposed FDA method to two dimensions would be useful for this type of data and could be done using two-dimensional spatial basis functions \citep[p. 84]{wikle_2019}. I also highlight in Sec.~\ref{sec:belt-wear-discussion} that the model I propose does not account for large-scale spatial dependence between the spline coefficients. When analysing a two-dimensional dataset with observations on a densely observed grid, it would be informative to incorporate large-scale spatial effects into the model. One possible way of doing this is to use a conditional autoregressive (CAR) prior.

\section{Implications for industry practitioners} \label{sec:thesis-practical}

Throughout this thesis, the Bayesian approaches I have presented address real problems in reliability data from the mining and mineral processing sector: retaining the information from lifetimes that are left-truncated with unknown exposure history, modelling multiple noisy degradation signals with gamma process models, and modelling the degradation of a wearing surface using functional data analysis. In addition to providing solutions to these practical problems, the approaches are implemented in Stan, an accessible and open-source probabilistic programming language, so that the models can be implemented and adjusted easily by a wide audience.

Throughout the work, I stressed the components of good Bayesian statistical workflow and demonstrated their application in the reliability domain. These include thinking carefully about how a model can be encoded in order to simplify its implementation; evaluating and refining the model and priors through simulation before fitting the data; interpreting uncertainty in the posterior through visualisation and posterior predictive distributions; propagating this posterior uncertainty through useful utility functions such as cost functions or the failure time distribution; and comparing sets of Bayesian models in a fully Bayesian framework using elppd and cross-validation methods. In the interests of reproducible research, I provide all of the code to implement the models and perform the analysis of this thesis on a Github repository \citep{code_repo} so that other reliability practitioners may follow along, reproduce and apply my work in practice, and adapt the models and workflow to their own industry problems.

Despite the rise of `big data' there are many reliability applications, particularly in mining and mineral processing, where practitioners are required to make well-informed decisions with data that is small, incomplete, noisy and/or messy. In these cases, it is important to squeeze as much information out of the dataset as possible; supplement analysis with whatever extra information is at hand, whether that be from the wealth of domain expert knowledge in these fields or historic data; and, most importantly, propagate the unavoidable, subsequent uncertainty from the analysis through any decision making framework that it informs.

To address the first two of these three points, I have have shown with respect to the lifetime data in Part~\ref{part:one} 
\begin{itemize}
  \item how when the likelihood for right-censored and left-truncated data is reframed in terms of missing data imputation, in the Bayesian context, when the exposure history of left-truncated samples is unknown we can retain the information in these lifetimes rather than discarding them; and
  \item how an extension of the method of \citet{kaminskiy2005} can be used to construct a joint prior for the Weibull parameters that encodes domain knowledge into different parts of the lifetime distribution to compensate for the missingness in the data.
\end{itemize}
In addition, I have have shown with respect to small, noisy degradation data in Part~\ref{part:two}
\begin{itemize}
  \item how to formulate a noisy gamma process model in a way so that it is straight-forward to encode prior information and specify models for multiple units, which allows information to be shared amongst the units;
  \item how a functional data interpretation of conveyor belt wear profiles allows us to model all of the wear measurements across the profile; and
  \item how past belt wear datasets can inform analysis of the current belt through an informative prior.
\end{itemize}
To address the third and final point, I have provided some concrete examples of constructing useful posterior predictive distributions for decision-making, such as the remaining useful life and cumulative failures of the idler frames in operation on the conveyor, and of propagating the posterior uncertainty through functions of the parameters, such as a cost function in Chap.~\ref{chap:chapter3} and failure time distributions in Chaps.~\ref{chap:chapter5} and~\ref{chap:chapter6}. Few works in the literature on Bayesian reliability provide guidance for practitioners on this last step in the workflow. More works in Bayesian reliability, like this one, should present examples of this last step in a Bayesian \emph{reliability} workflow.

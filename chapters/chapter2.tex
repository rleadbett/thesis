\chapter{Weibull analysis of partially observed lifetime data} \label{chap:chapter2}

Computerised maintenance management systems (CMMS) such as SAP \citep{sap} are now embedded in companies' maintenance procedures, meaning that these companies now possess large-scale datasets of component installation and replacement times. A natural use of these personalised failure time data sets is for tailoring replacement strategies for the company's specific operating environments \citep[p. 13]{Meeker2022}, rather than solely relying on the manufacturer's recommendations. One problem, however, is that these large observational datasets collected through CMMS are much messier than the experimental ones used by manufacturers in traditional reliability/warranty analysis. This messiness comes about because of reporting issues, incomplete historical records, and the fact that most components are pre-emptively replaced before they fail because of the risk to production and employee safety. The result is that many of the valuable data sets stored in CMMS systems are incomplete in the form of censoring and left-truncation. Censoring is when the true lifetime of a failed component is not known, but either an upper bound, lower bound, or both are known. On the other hand, left-truncation arises when only units that have lasted more than some truncation time are observed.

The censored and left-truncated nature of such data makes what would otherwise be a very straightforward analysis far more complicated. Worse yet, the incompleteness of the data is not always obvious, and mistreatment during analysis can lead to biased results and misinformed decisions. The idler-frame dataset in section~\ref{sec:industry-data} is one such case where data are both left truncated and right censored. In this case, right censoring arises due to the set of idlers in a frame either being preventatively replaced or still being in operation when the data were analysed, and some lifetimes are left-truncated since any idlers that were installed and failed before the time failures started being recorded in the CMMS are not present in the dataset. A further complicating factor of the idler frame dataset is that the installation times of idler-frames that were already in operation when data started being captured in the CMMS are unknown, meaning that the left-truncated lifetimes are also censored and have unknown truncation times. This issue is sometimes referred to as unknown initial conditions or unknown exposure history \citep{guo1993}. Treatment of right-censored and left-truncated data was addressed by \citet{hong2009}; however, not for cases with unknown exposure history of left-truncated samples. In this chapter, I propose a method for handling such cases in a Bayesian framework by imputing the unobserved portion of the left-truncated lifetimes with unknown exposure history and, along with them, the truncation times. I demonstrate the method using simulated data that mimics the observation process of the idler-frame data.

The incompleteness of a dataset caused by censoring and truncation reduces the information in the dataset and hence increases uncertainty in the analysis, particularly concerning the upper tail of the lifetime distribution since the data only contains partial information about the longer lifetimes. To reduce uncertainty in the analysis of incomplete lifetime data, domain knowledge can be used to fill the gap left by censoring, but only if the prior is constructed properly. \citet{kaminskiy2005} proposed a method for constructing a joint prior for the two parameters of the Weibull distribution by eliciting domain expert knowledge on the CDF. However, in their original paper \citeauthor{kaminskiy2005} only briefly introduce the method and do not explore how eliciting information at different parts of the CDF translates into the parameter space. Furthermore, they do not implement the model within a full lifetime model. I elaborate on the method for eliciting a joint prior by demonstrating how the choice of elicitation times along the CDF can be used to encode information at different locations in the distribution and implement the prior properly in a Bayesian lifetime model that incorporates both right censoring and left truncation. These developments on the method of \citet{kaminskiy2005} allow an analyst to supplement the analysis of left-truncated and right-censored lifetime data with a carefully constructed informative prior and hence `fill in the gaps' left by the incomplete lifetime data.

In the next section, section~\ref{sec:lifetime-data-background}, I provide a background of lifetime analysis, the Weibull distribution, and how censoring and truncation can be included in the likelihood of the data. I then describe my proposed approach for modelling left-truncated data with unknown exposure history by imputing the unobserved portions of the left truncated samples and their truncation time in section~\ref{sec:lt-imputation}. In section~\ref{sec:weibull-joint-prior}, I introduce the method proposed by \citet{kaminskiy2005} for constructing a joint prior, point out its limitations, and introduce my developments of the method. In section~\ref{sec:weibull-sim-example}, I demonstrate, using simulated data, my method for imputing partially observed left-truncated lifetimes and incorporating an informative joint prior. In this demonstration, I compare the imputation method alongside the case where we simply discard the partially observed left truncated lifetimes and a case where we fully observe them (if we know their installation times). Section~\ref{sec:weibull-sim-study} presents a small simulation experiment that explores the limitations of the imputation approach. The chapter concludes with section~\ref{sec:weibull-conclusion}, where I summarise the key contributions and findings from the chapter and provide recommendations for analysing lifetime datasets with right censoring and left truncated observations with unknown exposure histories.

\section{Background} \label{sec:lifetime-data-background}

Lifetime analysis, also called survival analysis, is the analysis of failure time data from a population of particular components/assets to derive the risk of failure of a component dependent on its level of exposure (usually some form of time) and sometimes other covariates \citep{moore2016}. From here on, I will use the general term unit/s to refer to individuals/groups of the same asset or component. Lifetime analysis of a population of units typically takes place by first specifying a sampling distribution for the lifetimes by choosing some parametric lifetime distribution for the units and incorporating any observational characteristics of the data---for example, censoring---then estimating the parameters of the distribution from failure time data using an appropriate inferential mechanism, and finally using the fitted model to derive useful reliability measures about the population which can be used to inform asset management plans. When done in a Bayesian context, the first step of this process also includes specifying a prior distribution. From the resulting inference, we can devise optimal replacement strategies that minimise the risk of unplanned failures and, hence, the risk of lost production, as well as the cost of the maintenance strategy.

\subsection{Lifetime distribution}

The lifetimes of the units are modelled as a random variable defined in terms of $t$, the exposure time, on $[0, \infty)$. $t$ is some continuous or discrete exposure time from a clearly defined origin, the installation of the component, to a well-defined event, the component's failure. In reliability analysis, the exposure is typically absolute time, the operating time of the unit, or cycles of operation. For example, the idler-frame failures are recorded in absolute time since operating time is unavailable. Next, a specific parametric lifetime distribution is chosen for the random variable $t$, $p(t|\theta)$, expressed as the probability density function (PDF) and the parameters of the lifetime distribution are estimated from the data. Once the estimates are obtained, different specifications of the lifetime distribution can be used to draw useful insights in order to inform decisions:
\begin{itemize}
    \item \textbf{Cumulative distribution function} (CDF), $F(t)$, it the probability that a unit will have failed by time $T$, i.e. $P(t <= T)$. It is also sometimes called the cumulative risk function.
    \item \textbf{Survival function}, $S(t)$, is the complement of the CDF and defines the probability of a unit surviving up to an exposure time $t$.
    \item \textbf{Hazard function}, $h(t)$, which is the instantaneous failure rate conditioned on the age of the unit.
\end{itemize}
For example, the CDF quantifies the risk of unplanned failures given a chosen preventative maintenance interval, and the Hazard function identifies if a unit's risk of failure increases as it ages and, therefore, if a preventative maintenance strategy is even suitable at all.

\subsection{The Weibull distribution}

In the analysis that follows, I use the Weibull distribution to model the idler frame lifetimes, that is
\begin{equation}
    y|\beta, \eta \sim \hbox{Weibull}(\beta, \eta),
\end{equation}
where $\beta$ is the shape parameter and $\eta$ is the scale. The Weibull distribution is a commonly used lifetime distribution because of its ability to capture an increasing, constant, or decreasing risk of failure. In addition, the Weibull distribution is the limiting distribution for the minimum value in a sample when the sample space is lower bounded; such as lifetimes, which must be greater than zero. This characteristic of the Weibull distribution gives it a convenient interpretation in component reliability; the lifetime of a unit is the time of the first occurring catastrophic failure mode of the unit. In my analysis that follows, I use the coupled parameterization of the two-parameter Weibull distribution, which has PDF
\begin{equation}
    f(t) = \frac{\beta}{\eta}\left(\frac{t}{\eta}\right)^{\beta - 1} \exp^{-\left(\frac{t}{\eta}\right)^{\beta}},
\end{equation}
CDF
\begin{equation}
    F(t) = 1 - \exp^{-\left(\frac{t}{\eta}\right)^{\beta}},
\end{equation}
and hazard function
\begin{equation}
    h(t) = \frac{\beta}{\eta}\left(\frac{t}{\eta}\right)^{\beta - 1}.
\end{equation}
The shape parameter $\beta$ dictates whether the hazard increases, $\beta > 1$, decreases, $\beta < 1$, or stays constant, $\beta = 1$. The effect of the shape parameter on the hazard function is demonstrated in Figure~\ref{fig:hazard_function_demo}. Practically speaking, if the hazard function increases with exposure, this corresponds to a wear-out failure mechanism, whereas if it decreases, it corresponds to infant mortality. This distinction is important from a maintenance perspective because if the component does not wear out, a preventative replacement policy is not suitable \citep{jardine2013}. In other words, we want to be sure that $\beta > 1$ before implementing a preventative policy.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{./figures/hazard_func_demo.pdf}
    \caption{The Weibull hazard function for $\beta = 0.9$, $\beta = 1$, and $\beta = 1.1$.}
    \label{fig:hazard_function_demo}
\end{figure}

\subsection{Censoring}

It is very common for lifetime data to be censored. Censoring occurs when we only partly observe the lifetime of a unit, or in other words, we only observe upper and lower bounds for the lifetime. There are three types of censoring: left, interval, and right censoring, but all are treated in much the same way. Figure~\ref{fig:cense_examp} demonstrates these three types of censoring. In the figure, three units are installed at time $t_0$, and then two follow-up inspections are performed at times $t_1$ and $t_2$. The true---partially observed---failure times of each unit are shown as crosses in the figure. Left censoring occurs when we only observe an upper bound of the lifetime. An example is if we know the install time of a unit and observe it as failed at $t_1$, and so we only know that the true value of the lifetime must be less than $t_1$. Interval censoring occurs if we only know an upper and lower bound for the lifetime, i.e. if a unit fails between inspection times, then we know that the lifetime must be greater than $t_1$ but less than $t_2$. Right censoring occurs when we only know a lower bound for the lifetime. For example, if a unit is still in operation when we perform our analysis, e.g. it has lasted longer than $t_2$, then we only know that the true value of the lifetime must be greater than $t_2$. Right and left censoring are special cases of interval censoring where the upper or lower bound of the lifetime is infinity or zero, respectively. Left censoring is fairly uncommon, so in the discussions that follow, I focus on right and interval censoring. However, all of the methods can be easily extended to accommodate left-censored data as well.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{./figures/censoring_example.pdf}
    \caption{An example of the different types of censoring. Three units are installed at $t = 0$, indicated by a black dot, and their failure times are shown as black crosses. The units are inspected at times $t_1 = 0.5$ and $t_2 = 1$. The portion of the lifetimes that is observed is shown as a solid black line, and the unobserved (incomplete) portion is shown as a dashed grey line.}
    \label{fig:cense_examp}
\end{figure}

One way of handling censored data is to treat the censored lifetimes as missing data, which in a Bayesian framework is to treat them as a random variable in the model \citep[p.211]{reich2019} and constrain their values to fall within the upper and lower censoring bounds \citep{stan_user_guide2024}. This is easily done in probabilistic programming languages like Stan. Treating the missing data as random variables in the model requires us to specify a distribution for the censored lifetimes, which we assume is the same distribution as the rest of the population,
\begin{equation}
    \label{eq:impute-cens}
    y^C_i|\beta, \eta \sim \hbox{Weibull}^{t^{\hbox{\tiny{Upper}}}_i}_{t^{\hbox{\tiny{Lower}}}_i}(\beta, \eta).
\end{equation}
Here $y^C_i$ is an unobserved censored lifetime, and the superscript $t^{\hbox{\tiny{Upper}}}$ and subscript $t^{\hbox{\tiny{Lower}}}$ indicate that the distribution is constrained by the upper and lower censoring times. Once the missing lifetimes have been imputed, the likelihood of the observed and imputed lifetimes can be calculated in the same way as a typical lifetime dataset with no censoring. This approach of imputing the censored lifetimes is not unique to Bayesian methods. The same can be done using an Expectation Maximisation algorithm and maximum likelihood \citep{mitra2013}. However, using the Bayesian approach, along with MCMC methods, it is very simple to derive uncertainty intervals for the parameters, imputed values, and useful quantities.

An alternative approach is to simply integrate out the censored observations. The probability that a censored observation falls between the upper and lower censoring times is
\begin{equation}
    \label{eq:integrate-out-cens}
    Pr\left[t^{\hbox{\tiny{Lower}}} < y^C_i \leq t^{\hbox{\tiny{Upper}}}\right] = \int_{t^{\hbox{\tiny{Lower}}}}^{t^{\hbox{\tiny{Upper}}}} f\left(y^C_i\right) d y^C_i = F\left(t^{\hbox{\tiny{Upper}}}\right) - F\left(t^{\hbox{\tiny{Upper}}}\right).
\end{equation}
By integrating out the censored observations, the likelihood can be written as
\begin{equation}
    \label{eq:censored_likelihood}
    L\left(\theta|y^O, t^{\hbox{\tiny{Upper}}}, t^{\hbox{\tiny{Lower}}}\right) = \prod^{N^O}_{i = 1}f(y^O_i) \prod^{N^C}_{j = 1}\left[F(t^{\hbox{\tiny{Upper}}}_j) - F(t^{\hbox{\tiny{Lower}}}_j)\right],
\end{equation}
where $\theta$ are the parameters of the lifetime distribution, $N^O$ and $N^C$ are the number of fully observed and censored observations, respectively, and $y^O_i$ are the fully observed lifetime. This second approach is much more commonly used, particularly in the reliability literature (for example \citet{Meeker2022,tian2024,hong2009,mittman2013}). However, as I show later, it is convenient to frame the model using the first imputation approach for the particular problem when data are also left-truncated with unknown installation times.

\subsection{Left-truncation}

Truncation arises when a sample comes from an incomplete population, or in other words, there is some criteria that part of the population must appease in order to be observable \citep{guo1993}. Left-truncation, for example, arises when some units must survive up to a certain time to be included in the dataset. It is also possible for data to be right- or doubly-truncated, but left-truncation is the most common in lifetime data. The definition of left-truncation and left-censoring may seem very similar; however, they are distinctly different \citep{mitra2013}. Censoring is a characteristic of the sample, i.e. we know the number of left-censored observations but not the exact values of their lifetimes. In contrast, truncation is a characteristic of the population because we do not know how many units were not included in the dataset because they did not survive past the truncation time (the time from the installation of the unit to the start of observation). Hence, our sample is not representative of the true population. An example of a left-truncated dataset is shown in Figure~\ref{fig:left_trunc_example}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{./figures/left_truncation_example.pdf}
    \caption{An example of a dataset with left-truncated samples. Units are put under test at random times, and their failure times are recorded. However, the unit is only captured in the dataset if it fails after $t_{start}$; therefore, any units that were installed and failed before $t_{start}$ are not included in the sample.}
    \label{fig:left_trunc_example}
\end{figure}

Figure~\ref{fig:left_trunc_example} shows an example case where units are put on test at random times, and their failures are recorded after some time $t_{start}$. For all units that fail after $t_{start}$, their installation time is known; however, any units that failed before $t_{start}$ are absent from the dataset. The example is similar to the transformer lifetime dataset analysed by \citet{hong2009}. In Fig.~\ref{fig:left_trunc_example}, units that were installed prior to $t_{\hbox{start}}$ come from a left-truncated distribution since any other unit that was installed at the same time but did not last until $t_{\hbox{start}}$ are not included in the sample. The left-truncated cases caused by the start of the observation period (units three, four, five, and seven in Fig.~\ref{fig:left_trunc_example}) tend to over-represent low-risk cases since any high-risk case installed at the same time is absent from the sample \citep{guo1993}. Observations that arise from a left-truncated distribution can be included in the analysis by re-normalising their lifetime distribution by dividing by the probability of surviving past the truncation time;
\begin{equation}
    \label{eq:left_trunc}
    L\left(\theta|y^T_i\right) = \frac{f\left(y^T_i\right)}{1 - F\left(\tau^{\hbox{\tiny{L}}}_i\right)},
\end{equation}
where $y^T_i$ is the left-truncated lifetime, and $\tau^{\hbox{\tiny{L}}}_i$ is the truncation time.

\subsection{Left-truncation and right-censoring} 

A common scenario in reliability datasets is a combination of both left-truncation and right-censoring. This case naturally arises in historic observational datasets, such as those found in CMMS, where units are repeatedly replaced once they fail, and any units that were installed and failed before the start of the observation process (which might be the date a new CMMS was adopted) are absent in the dataset. Figure~\ref{fig:left_trunc_and_right_cens_example} shows a toy example of this case, where three units are repeatedly replaced when they fail, and we start to observe their failures at $t_{\text{start}}$ and stop at $t_{\text{end}}$. Any lifetimes that fail before $t_{\text{start}}$ are unobserved---and greyed out in the figure---resulting in the first observed lifetime of each unit being a left-truncated sample. Lifetimes that surpass $t_{\text{end}}$ are only partially observed (right censored), hence the portion of these lifetimes that sits to the right of $t_{\text{end}}$ is also greyed out. \citet{hong2009,mitra2013,kundu2016} analyse a dataset of electrical transformer failures that follows this general structure, and \citet{mittman2013} looks at a similar case for computer hard drives. The idler frame failure data is also an example of this type of dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{./figures/left_truncation_w_right_censoring_example.pdf}
    \caption{An example of how left truncation and right censoring arise from the repeated replacement of three units. Three units are shown on the vertical axis, and time on the horizontal. The failure times of the units are shown as black crosses, and every time a unit fails, it is instantaneously replaced. If we start observing these failures at $t_{start}$ and stop at $t_{end}$, then any lifetime that began before $t_{start}$ and failed after is left truncated, and any lifetimes that began before $t_{end}$ and failed after are right censored.}
    \label{fig:left_trunc_and_right_cens_example}
\end{figure}

\citet{hong2009} shows how a likelihood for data that is left-truncated and right-censored can be constructed using the general approach of integrating out the censored observations,
\begin{equation}
    \label{eq:left_trunc_and_right_cens_int}
    L\left(\theta|y^O, t^{\hbox{\tiny{Lower}}}, \tau^L_i\right) = \prod^{N^O}_{i = 1}\left[\frac{f(y^O_i)}{1 - F(\tau^L_i)} \right] \prod^{N^C}_{j = 1}\left[\frac{1 - F(t^{\hbox{\tiny{Lower}}}_j)}{1 - F(\tau^L_j)}\right],
\end{equation}
where $\tau^L$ is the truncation times, and if the lifetime is not truncated, $\tau^L = 0$. \citet{kundu2016} then implements the same approach in a Bayesian framework using a Gibbs sampling algorithm to draw samples from the posterior. \citet{mitra2013} takes the alternative approach of imputing the censored lifetimes using an expectation maximisation algorithm and the complete data likelihood
\begin{equation}
    \label{eq:left_trunc_and_right_cens_imp}
    L\left(\theta|y^O, \hat{y}^C, \tau^L_i\right) = \prod^{N^O}_{i = 1}\left[\frac{f(y^O_i)}{1 - F(\tau^L_i)} \right] \prod^{N^C}_{j = 1}\left[\frac{f(\hat{y}^C_j)}{1 - F(\tau^L_j)}\right],
\end{equation}
where the $\hat{y}^C_j$ are the imputed values of the censored observations. I express the same approach as \ref{eq:left_trunc_and_right_cens_imp} in a Bayesian framework as
\begin{align*}
    y^O_i|\beta, \eta & \sim \hbox{Weibull}(\beta, \eta) \quad T[\tau^L_i, ]\\
    y^C_j|\beta, \eta & \sim \hbox{Weibull}_{t^{\hbox{\tiny{Lower}}}_j}(\beta, \eta) \quad T[\tau^L_j, ] \\
    \beta, \eta & \sim \pi(\theta_{\beta, \eta}),
\end{align*}
where $\hbox{Weibull}_{t^{\hbox{\tiny{Lower}}}_j}$ indicates that the random variable $y^C_j$ has a Weibull distribution and is constrained to be greater than the censoring time, and $T[\tau^L, ]$ indicates that the distributions are re-normalised by the probability $P(y > \tau^L)$ (Note: I stole this notation from the stan code). For the moment, I express the joint prior for the parameters in its most general form.

\paragraph{Unknown truncation time}

A problem arises when the installation time of the left-truncated lifetimes is unknown since to normalise the truncated lifetime distribution of the left-truncated observation $\tau^{\hbox{\tiny{L}}}$ must be known. For example, if for the dataset shown in Fig.~\ref{fig:left_trunc_and_right_cens_example} there was no information at all prior to $t_\text{start}$, then we could not use the likelihood in \eqref{eq:left_trunc_and_right_cens_int} or \eqref{eq:left_trunc_and_right_cens_imp}. This is the case for the idler frame data. This problem is either known as unknown exposure history or initial conditions \citep{guo1993}. In these cases, two approaches can be taken. The first is to discard all the left-truncated samples, in which case the parameter estimates are still unbiased. However, in doing so, we throw away a large amount of information. In most cases of left-truncation and right censoring, the right censoring masks any information about longer lifetimes, so the left-truncated samples are the only source of information about the upper tail of the lifetime distribution. The second approach is to assume a constant hazard, i.e. $\beta = 1$ since, in this case, the Weibull distribution reduces to the exponential and, no matter the age of a unit, the probability of it surviving a given period is constant (this is the memoryless trait of the exponential distribution). However, assuming a constant hazard is very restrictive and often, one of the aims of performing lifetime analysis in the first place is to determine if $\beta > 1$. Furthermore, assuming an exponential distribution when the data do not have a constant hazard may lead to severe bias in the parameter estimates \citep{heckman1986}. In Section~\ref{sec:lt-imputation}, I show how, by treating the unknown installation times as a case of censoring and imputing the censored data, the missing truncation times can also be imputed, and reasonable parameter estimates can be obtained.

\section{Imputing truncation times} \label{sec:lt-imputation}

Using the toy example in Fig.~\ref{fig:left_trunc_and_right_cens_example}, say we do not observe any of the installation or failure times to the left of $t_\text{start}$. In this case, we know that the first, partially observed lifetime from each unit started sometime between $t = 0$ and $t = t_\text{start}$. This is a case of interval censoring, where the lower censoring bound is the time from the beginning of observation to the failure time, and the upper bound is from $t = 0$ to the failure time. If we did not know the origin time $t = 0$ with respect to $t = t_\text{start}$, then it would be a case of right censoring, but the following logic would still apply. Let $t^{\text{failure}}_{i}$ be the failure time of the $i^{th}$ observation. Treating the observations as interval-censored, the left-truncated lifetime can be imputed as in \eqref{eq:censored_likelihood} by sampling from
\begin{equation}
    \hat{y}^L_i|\beta, \eta \sim \hbox{Weibull}^{t^{\text{failure}}_{i}}_{t^{\text{failure}}_{i} - t_\text{start}}(\beta, \eta).
\end{equation}
Using the imputed values of the lifetime, it is then possible to calculate the truncation time by
\begin{equation}
    \tau^L_i = \hat{y}^L_i - \left(t^{\text{failure}}_{i} - t_\text{start}\right).
\end{equation}

A complication arises when the lifetime is both interval censored by the start of the observation period and right censored by the end, such as unit one in Fig.~\ref{fig:left_trunc_and_right_cens_example}. In this case, the truncation times value is unknown but is between $\tau^L = 0$ and $\tau^L = \min\left(t_\text{start}, \hat{y}^L_i - (t_\text{end} - t_\text{start})\right)$. Since we have no reason to expect that the truncation time is not uniform, we can impute the truncation time using the uniform distribution
\begin{equation}
    \label{eq:imp-trun-times}
    \tau^L_i \sim \hbox{Uniform}(0, \min\left(t_\text{start}, \hat{y}^L_i - (t_\text{end} - t_\text{start})\right)).
\end{equation}
Sampling the value of $\tau^L_i$ in this way should incorporate the extra uncertainty in our Bayesian estimates.

\section{Informative joint prior} \label{sec:weibull-joint-prior}

In cases where data only provide partial information---such as lifetime data where some lifetimes are masked by censoring---an informative prior can help to `fill in the gaps' and inform areas of the model that the data cannot. For example, right censoring masks the upper tail of the lifetime distribution, and if there are no observed failures beyond some censoring time, then the data do not contain any information that informs this upper tail of the lifetime. This is clear in Section~\ref{subsec:weibull-model-fits} when I fit a Weibull distribution to censored lifetimes after discarding any truncated lifetimes. An added motivation for using an informative prior in these cases is that non-informative priors can place mass in unreasonable parts of the parameter space, and when combined with a weak likelihood, this leads to spurious parameter estimates \citep{tian2024}.

\citet{kaminskiy2005} proposes a method for encoding an informative joint prior for the two parameters of the Weibull distribution by eliciting information about cross sections of the Weibull CDF. In their proposed method, the analyst provides their expected value of the CDF at two exposure times, $t_1$ and $t_2$, and their level of uncertainty around these estimates. The pair of estimate and uncertainty level at each exposure time $(\mu_{\hat{F}_{t_i}}, \sigma_{\hat{F}_{t_i}})$ are encoded as the mean and standard deviation of a beta distribution. By then sampling realisations of the CDF at each exposure time from the two distributions, ensuring that $\hat{F}_{t_1} < \hat{F}_{t_2}$, the parameters of the Weibull CDF that passes through the two realisations can be calculated to obtain a draw from the informative joint prior. \citeauthor{kaminskiy2005} then show how to obtain a Bayesian point estimate of the CDF at some new exposure time where binomial failure data are available by first calculating the parameters of a new beta distribution that describes the CDF at the new exposure time using the joint prior draws and then updating the parameter estimates with the data, taking advantage of the beta-binomial conjugacy.

The method that \citeauthor{kaminskiy2005} propose to obtain joint draws from an informative prior for the two Weibull parameters falls under the recommendations of \citet{gelman_workflow_2020}, who suggest to elicit information on the outcome space---which is more familiar to partitioners---and then translate this information into an informative prior in the parameter space that also indirectly describes how the parameters should be allowed to covary with one another. An alternative method proposed by \citet{Meeker2022} to elicit an informative prior for the Weibull parameters is to reparameterise the distribution in terms of the shape $\beta$ and some quantile, $p_r$, of the distribution and then specify independent priors for $\beta$ and $p_r$. Both methods have proven useful in practice and have been implemented in reliability software due to their usability \citep{krivtsov2017}. However, it can be difficult to elicit information about $\beta$ for practitioners unfamiliar with Weibull analysis. Therefore, I use a variation of \citeauthor{kaminskiy2005}'s method for obtaining draws from a joint informative prior for the Weibull parameters and build upon their method by elaborating on what it means to elicit information at different values of $t_1$ and $t_2$ and how the translates into covariance in the joint distribution and implement the joint prior in a lifetime model so that the draws from the prior are properly filtered through the likelihood.

In their proposed procedure, the prior is updated with the data using the conjugacy of the binomial likelihood and beta prior for the CDF at the new exposure time $t_3$. Doing so only provides point estimates of the updated parameter values of the beta distribution at $t_3$. To obtain corresponding values of the Weibull parameters, the analyst must once again sample pairs of realisations along the CDF---now at either $t_3$ and $t_1$ or $t_3$ and $t_2$---and calculate the parameters of the Weibull CDF that passes through the two points. In doing so, they `reuse' the prior distribution at either $t_1$ or $t_2$ to generate the posterior, and so the `prior belief' about the CDF at whichever time is used is not really updated at all. Furthermore, the resulting joint draws will be sensitive to whether $t_1$ or $t_2$ is used to regenerate the joint distribution of the Weibull parameters. By instead implementing the method for obtaining draws from the joint prior within the HMC sampling algorithm, I show how to properly filter the prior through the likelihood to obtain the full posterior. Doing so also means that the prior can be updated with detailed lifetime data (i.e. failure times rather than binomial trial data), censoring and truncation information can be included in the likelihood, and the resulting full posterior contains proper uncertainty quantification. 

Rather than beta distributions, I use normal distribution truncated between zero and one to encode prior belief about the CDF at $t_1$ and $t_2$. This is because when specifying prior information close to the boundary, for example, if our estimate of the cdf is $0.95$ or $0.05$, with reasonable uncertainty surrounding the estimate, a beta distribution places a lot of mass very close to zero or one which can cause numerical issues during sampling. Truncated normal distributions are much better behaved. The prior can be expressed as
\begin{align*}
    \hat{F}_{t_1} \sim & \hbox{N}^{1}_{0}(\mu_{\hat{F}_{t_1}}, \sigma_{\hat{F}_{t_1}})             \\
    \hat{F}_{t_2} \sim & \hbox{N}^{1}_{\hat{F}_{t_1}}\left(\mu_{\hat{F}_{t_1}}, \sigma_{\hat{F}_{t_1}}\right) \\
    \beta            = & \frac{g(\hat{F}_{t_2}) - g(\hat{F}_{t_1})}{\log(t_1 / t_2)}               \\
    \eta             = & \exp\left[\log(t_1) - \frac{\hat{F}_{t_1}}{\beta}\right]
\end{align*}
where $g(\hat{F}) = \log(-\log(1 - \hat{F}))$ and the superscript and subscripts on the normal distribution indicate the upper and lower constraints of the support.

Depending on the choice of $t_1$ and $t_2$, the analyst encodes different information into the joint prior, which is reflected in how the two parameters covary with one another. For demonstration, plots (a), (b), and (c) in Figure~\ref{fig:kaminskiy-join-priors} show the draws from three different joint priors constructed using this elicitation method. To construct the prior, I specify some `true' values of the parameters $(\beta = 1.1; \eta = 1)$ and then specify the priors to reflect the true value of the CDF at different quantiles $t_i = p_r$ so that all of the priors contain the `true' parameter values. (a) shows a prior where information is encoded at early exposure times ($t_1 = p_{0.05}$ and $t_2 = p_{0.20}$), (b) shows one where information is encoded at times around the median age ($t_1 = p_{0.40}$ and $t_2 = p_{0.60}$), and (c) shows one where the elicitation times are in the upper tail of the lifetime distribution($t_1 = p_{0.90}$ and $t_2 = p_{0.99}$). Plots (d), (e), and (f) in Figure~\ref{fig:kaminskiy-join-priors} show the corresponding uncertainty around the Weibull CDF that results from the different priors in (a), (b), and (c) respectively. We see that the joint prior allows us to express our prior belief in one area of the PDF while allowing the prior to be vague in others. This characteristic has useful application in the context of heavy censoring since censoring masks longer lifetimes and hence increases uncertainty in the upper tail of the distribution but still contains useful information in the lower tail.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{./figures/ch-2/joint-priors.pdf}
    \caption{Three different informative joint priors constructed using the method of \citet{kaminskiy2005}. The joint draws of the parameters are shown in the top row---(a), (b), and (c)---and the corresponding uncertainty in the CDF is shown in the bottom---(d), (e), and (f). (a) and (d) show a prior where information is elicited around the $t_1 = 0.07$ and $t_2 = 0.26$ (the 0.05 and 0.20 quantiles of the true distribution $\hbox{Weibull}(1.1, 1)$), (b) and (e) show a prior where information is elicited around the $t_1 = 0.46$ and $t_2 = 1.04$ (the 0.35 and 0.65 quantiles), and (c) and (f) show a prior elicited at $t_1 = 1.54$ and $t_2 = 2.71$ (the 0.80 and 0.95 quantiles).}
    \label{fig:kaminskiy-join-priors}
\end{figure}

In this chapter, I compare the informative joint prior shown in Figure~\ref{fig:kaminskiy-join-priors}~(c) and~(f), which encodes information in the upper tail, with the vague independent priors $\beta \sim \hbox{N}^{+}(1.1, 1)$ and $\eta \sim \hbox{N}^{+}(1, 1)$, where the superscript $(+)$ indicates that the prior is truncated to be positive. 

\section{Analysis of simulated data} \label{sec:weibull-sim-example}

In this section, I demonstrate the proposed method for imputing the left-truncated samples with unknown exposure time using data simulated in a way that emulates the idler-frame observation process. I simulate the repeated replacement of a set of units and impose left truncation and right censoring by recoding the failures that occur within an observation period. I then fit a Bayesian model, which imputes any partially observed lifetimes in the dataset. I compare the results with two alternative analyses, one where the left-truncated samples are discarded, and only the fully observed and right-censored observation are used to estimate the Weibull distribution, and another where the left-truncated lifetimes are fully observed; this second analysis is the ideal scenario. I compare the three different cases using both vague independent priors for the Weibull parameters and an informative joint prior.

For a small sample, the imputation approach gives almost the same inference as the case where we know the true installation times. When we throw out all of the left truncated observations, there is a massive increase in uncertainty, particularly around the upper tail of the lifetime distribution. However, if an informative joint prior is carefully constructed to inform the upper part of the distribution, then the results of the three different analyses are very similar. In subsection~\ref{subsec:sim-method-weibull}, I explain how I simulate data that emulates the data-generating process of the idler-frames. I then analyse the simulated data in subsection~\ref{subsec:weibull-model-fits} using the three different approaches and a vague prior and compare the posteriors. In subsection~\ref{subsec:weibull-model-fits-informative}, I repeat the three analyses using an informative joint prior and compare the posterior distributions again.

\subsection{Simulation method} \label{subsec:sim-method-weibull}

To simulate data that emulates the idler-frame lifetime data set, I sample $N \times M$ draws from a Weibull distribution with known shape parameter $\beta = 1.1$ and scale parameter $\eta = 1$. I then assign these lifetimes to $M = 10$ units. To calculate failure times rather than lifetimes, I take the cumulative sum of the $N$ lifetimes assigned to each unit. The installation times are calculated by taking the lag of the failure times. I then define a start, $t_{start} = 5$, and end, $t_{end} = 6$, time for the observation window. Any lifetimes where both the install and failure times sit either before $t_{start}$ or after $t_{end}$ are discarded. Of the remaining lifetimes, if the install time is less than $t_{start}$, then $t_{start}$ is set as the observed start of the lifetime and the observation is marked as left truncated. At the same time, if the failure time is greater than $t_{end}$, then $t_{end}$ is substituted as the observed failure time, and the observation is marked as right censored. Table~\ref{tab:sim-cmms-data} presents a simulated dataset using this method and parameter values. The simulated observations are also plotted in Figure~\ref{fig:sim_censored_units}, where the start and end of observation are shown as red vertical lines, and the observed portions of the simulated data are shown in black and the unobserved `incomplete' part is shown in grey. Notice that if we were to throw away the left truncated samples, then we would be discarding more than 50\% of the sample.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{./figures/ch-2/sim-data.pdf}
    \caption{The set of simulated lifetimes data. The exposure is shown on the horizontal axis, and the beginning and end of the observation period are shown as red vertical lines. The observed portions of the lifetimes are shown as solid black lines, while the unobserved portions are shown as dashed grey lines.}
    \label{fig:sim_censored_units}
\end{figure}

\input{./tables/ch-2/sim_cmms_data.tex}

\subsection{Weakly informative prior} \label{subsec:weibull-model-fits}

I now fit the imputation model to the simulated data in Table~\ref{tab:sim-cmms-data} and compare the resulting posterior with the case where the installation times of the left truncated samples are known and also with the alternative treatment of the unknown exposure histories, which is to discard all of the left truncated samples. I fit these models with weakly informative, independent priors
\begin{align*}
    \hat{F}_{t_1} \sim & \hbox{N}^{+}\left(1.1, 1\right)  \\
    \hat{F}_{t_2} \sim & \hbox{N}^{+}\left(1, 1\right).
\end{align*}
The three stan models are available on the GitHub repository. To sample from the posteriors, I use four chains, each 1000 iterations long with a burn-in of 500. Figure~\ref{fig:joint-post-weibull} shows the three posteriors. Plots (a), (b), and (c) show the joint draws of the two Weibull parameters for the fully observed, discarded, and imputed treatment of the left truncated samples, respectively. The corresponding CDFs with uncertainty are shown in (d), (e), and (f). The true values of the parameters and the true CDF are plotted in red in the plots. The resulting inference from imputing the installation times of the left truncated samples is almost the exact same as if we had fully observed the left truncated samples. Discarding the left truncated samples results in a much more diffuse posterior and, consequently, more uncertainty around the CDF, particularly for larger exposure times. Next, I refit the models using a slightly more diffuse version of the informative prior shown in Fig.~\ref{fig:kaminskiy-join-priors}~(c) and~(f), which informs the upper tail of the distribution.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{./figures/ch-2/joint-posts.pdf}
    \caption{The draws from the joint posteriors conditioned on the simulated dataset when the left-truncated lifetimes are fully observed (a), discarded (b), or imputed (c) and a weakly informative prior is used. (d), (e), and (f) show the corresponding uncertainty around the CDF (in the form of the 0.5 and 0.8 uncertain intervals) that result from (a), (b), and (c), respectively. The true parameter values and CDF are shown in red.}
    \label{fig:joint-post-weibull}
\end{figure}

\subsection{Strongly informative prior} \label{subsec:weibull-model-fits-informative}

The prior in Fig.~\ref{fig:kaminskiy-join-priors}~(c) and~(f), which elicits information about the CDF at $t_1 = p_{0.80} = 1.54$ and $t_2 = p_{0.95} = 2.71$ encodes an informative prior that strongly informs the Weibull model in the upper tail of the distribution but is sufficiently vague in the lower tail, where the data are strongly informative. Here, I refit the Weibull models with a slightly more diffuse version of the prior; i.e. I increase the uncertainty around the estimates of the CDF at each elicitation time. The informative prior is
\begin{align*}
    \hat{F}_{t_1} \sim & \hbox{N}^{1}_{0}\left(0.8, 0.1\right)    \\
    \hat{F}_{t_2} \sim & \hbox{N}^{1}_{0}\left(0.95, 0.05\right).
\end{align*}
Using the informative prior, I refit the three models from Section~\ref{subsec:weibull-model-fits}. Once again, I use four chains, each 1000 iterations long, with a burn-in of 500 to sample from the three posteriors. The resulting draws are plotted in (a), (b), and (c) of Figure~\ref{fig:joint-post-weibull-inf} for the fully observed, discarded, and imputed treatment of the left truncated samples, respectively. Comparing the posterior of the imputation method (plots~(c)) with the case where the left truncated samples are fully observed (plots~(a)), the resulting inference is again very similar. However, using the informative prior, the posterior of the case where I have discarded the left truncated samples, plot~(b), is much closer to the other two posteriors than when a vague prior is used. The corresponding posterior CDFs are shown in (d), (e), and (f) of Fig.~\ref{fig:joint-post-weibull-inf}. The uncertainty in all cases has been refined, but most drastically in the upper right part of the CDF. In Fig.~\ref{fig:joint-post-weibull-inf}~(e), the effect of combining a likelihood that informs the CDF at lower exposure times with a prior that informs the CDF at higher exposure times has resulted in a precise estimate of the CDF. 

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{./figures/ch-2/joint-posts-inf.pdf}
    \caption{The joint posteriors conditioned on the simulated dataset when the left-truncated lifetimes are fully observed (a), discarded (b), or imputed (c) and an informative joint prior is used that encodes information into the upper tail of the lifetime distribution. (d), (e), and (f) show the corresponding uncertainty around the CDF (in the form of the 0.5 and 0.8 uncertain intervals) that result from (a), (b), and (c), respectively. The true parameter values and CDF are shown in red.}
    \label{fig:joint-post-weibull-inf}
\end{figure}

In the implementation of the joint prior that I have used, the prior is properly updated through the MCMC routine. i.e. the observed data have slightly updated our belief about the value of the CDF at both $t_1$ and $t_2$. Figure~\ref{fig:weibull-prior-post-comp} compares the distribution of the posterior draws of $\hat{F}(t_i)$ (grey densities) with the prior (red curves). In the three plots in Fig.~\ref{fig:weibull-prior-post-comp}, the posterior distributions are clearly narrower than the prior, more so at $t_1$. Fitting the Weibull models to the data in Table~\ref{tab:sim-cmms-data} has shown that, for small sample sizes, in cases where we do not know the exposure history of the left truncated samples the imputation method provides almost equivalent inference to if we had fully observed the left-truncated samples. Furthermore, if we alternatively `throw-away' the left truncated observations, this results in a large loss of information; however, this loss can be compensated for if there is prior information about longer lifetimes and this information is properly encoded into a joint prior. Here, I have shown one case for a reasonably small sample. Next, I devise a small simulation experiment to more rigorously evaluate the imputation method for the left truncated samples with unknown exposure history.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/ch-2/prior-post-comp.pdf}
    \caption{Comparison of the marginal prior and posterior for $F_{t_1}$ and $F_{t_2}$ when left-truncated lifetimes are fully observed (a), discarded (b), or imputed (c) to show how both the elicited distributions have been updated in the posterior.}
    \label{fig:weibull-prior-post-comp}
\end{figure}

\section{Simulation experiments} \label{sec:weibull-sim-study}

To explore the behaviour of the imputation method for left truncated lifetimes with unknown exposure history, I repeat the simulation and model fitting process of Section~\ref{sec:weibull-sim-example} for a number of different combinations of the simulation parameters: number of units, start of observation time, and length of observation time. I compare the imputation method---under both a vague and informative prior---with the alternative method of discarding the left truncated observations as well as with the case where the left truncated samples are fully observed (as ground truth). First, I define the factor levels of each simulation parameter that make up the factor combinations in the simulation experiments. Next, I describe the measures of model accuracy that I use to compare the three models. Finally, I present the results of the simulation experiments.

\subsection{Factor levels}

I vary three factors when simulating the datasets, each with three levels;
\begin{itemize}
    \item $N$: 10, 100, 500 units
    \item $t_{start}$: 1, 5, 15 mean lifetimes
    \item $t_{end} - t_{start}$: 1, 3, 6 mean lifetimes
\end{itemize}
Increasing the number of units, $N$, increases the sample size, increasing $t_{start}$ increases the range of possible values the left truncated samples with unobserved truncation time could take---and therefore reduces the information they contribute---and increasing the window size, $t_{end} - t_{start}$, increases the number of fully observed lifetimes, the maximum length of the fully observed lifetimes and the number of samples that are both left truncated and right censored. For each factor combination, I perform one hundred simulations, each time fitting all three treatments for the left-truncated observations with both a vague and informative prior.

\subsection{Accuracy measures}

To compare how well the Bayesian models reclaim the true data generating mechanism, I calculate the Bayesian p-values of the true parameter values---$\beta = 1.1$ and $\eta = 1$---under the fitted posteriors and the $elppd$ of a dataset of one hundred fully observed lifetimes generated from the true Weibull distribution. The Bayesian p-value for beta is easily calculated using the posterior draws as
\begin{equation*}
    p_{\beta} = \frac{\sum^{S}_{s = 1}{\beta_{true} \le \beta_s}}{S},
\end{equation*}
similarly, the P-value of $\eta$ can be calculated. If the model posterior is well calibrated, then the p-values from repeating the same simulation should have a uniform distribution. If the parameter estimates are biased, then the P-values should, on average, sit above or below $0.5$. If the posterior uncertainty is overly diffuse, then the p-values will be clustered around $0.5$, and if the uncertainty is too narrow, then the P-values will be pushed towards zero or one. The P-value provides an indication that the model is over or under-predicting the parameter values but does not express to what degree.

To determine the scale of any discrepancy, I also calculate the $elppd$ of a new sample of one hundred fully observed lifetimes under the different posteriors. I do this by simulating one hundred datasets, each with one hundred observations and calculating the expected log-likelihood of each simulated dataset and then taking the average,
\begin{equation*}
    \label{eq:elppd-100}
    elppd_{100} = \frac{\sum_{n = 1}^{100}\frac{1}{S}\sum_{s = 1}^{S}\sum_{i = 1}^{100}f(\tilde{y}_{n, i}|\beta_s, \eta_s)}{100}.
\end{equation*}

\subsection{Results}

The simulation experiment results show that under some circumstances, there is a slight bias in the posterior estimates of the parameters when the partially observed left-truncated lifetimes are imputed. There are two separate causes of the bias; the first comes from the improper treatment of lifetimes that are both left truncated by the beginning of the observation period and right censored by its end, and so only impacts inference when the observation period is short ($t_{end} - t_{start} = 1 \text{mean lifetime} = 0.95$); the second occurs because the uncertainty expressed in the posterior is an expression of \textit{our} uncertainty and so is not equivalent to frequentist credible intervals, and so the p-values appear bias. The scale of this second source of bias appears less extreme. Despite these two sources of bias, $elppd$ results suggest that in some cases, it is better to use the slightly bias treatment in favour of discarding the left-truncated lifetimes because the precision of the posterior places more mass around the true data-generating mechanism, particularly if a weak prior is used.

Figure~\ref{fig:sim-study-pvalue} summarises the p-values of the 100 simulation runs under each factor combination by their means. The different treatments of the left-truncated lifetimes---fully observed, imputed, or discarded---are shown as different colours and the type of prior---informative or vague---are shown as different point types, for example, fully observed left-truncated lifetimes and a strongly informative prior is shown as a blue dot. If the model's posterior is unbiased, then the point should sit in the centre of each plot, i.e. an average p-value of $0.5$ for both parameters. If the average of the p-vales is at one or zero, then there is typically very little posterior mass around the true parameter value, and hence, we can conclude that the inference is biased. When $N = 10$, the bias in parameter estimates is negligible compared to the uncertainty in the posterior, so all of the different treatment-prior combinations perform fairly similarly. However, as the number of units increases and the sample size gets larger, the posteriors become more precise, and so the bias becomes more obvious.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/ch-2/sim-results-pvalues.pdf}
    \caption{The simulation results are summarised by the mean of the one hundred repetitions for each factor combination. The columns show the three levels of $t_{end} - t_{start}$, and the rows show the levels of $t_{start}$ and $N$. The Bayesian p-values of $\beta$ and $\eta$ are reported on the horizontal and vertical axis, respectively, and the colours of the points show the treatment of the left-truncated samples, and the shape shows the prior. The size of the points correspond the the average $elppd_{100}$ calculated according to \eqref{eq:elppd-100}}.
    \label{fig:sim-study-pvalue}
\end{figure}

The leftmost column in Fig.~\ref{fig:sim-study-pvalue} shows the simulations where the window size is equal to one mean lifetime, $t_{end} - t_{start} = 0.95$. In this case, there is a high proportion of lifetimes that begin before the start of the observation period and end after the observation period---i.e. they are both left-truncated at the beginning and right-censored at the end. For these cases, the Bayesian p-value of the shape parameter, $\beta$, is greater than $0.5$ and depending on the value of $t_{start}$ the scale parameter $\eta$ is either over ($p_{\eta} > 0.5$) or under ($p_{\eta} < 0.5$) estimated. Bias, when $t_{start}$ is small, is expected since the assumption of uniform entry, I make in the model in \eqref{eq:imp-trun-times} is not valid since there is a higher chance that lifetimes were installed at $t_0$. However, the bias in the cases where $t_{start} << 0$ is unexpected. Inspecting the posteriors more closely shows that the HMC algorithm is updating the random variable $\tau^L$ in \eqref{eq:imp-trun-times}, where it should instead be treated as a nuisance parameter and excluded from the updating procedure. For the cases where the $t_{end} - t_{start} > 1$ mean lifetime, the proportion of lifetimes that are both left truncated and right censored is low, and hence, so is the bias caused by these observations. 

In Fig.~\ref{fig:sim-study-pvalue}, when $t_{end} - t_{start} > 1$ (the two right-most columns), the posterior where the left-truncated lifetimes are imputed is much more aligned with the other two treatments. However, as the number of units is increased, a slight underestimation of the shape parameter $\beta$ becomes apparent when $t_{start} >> 0$. However, even for the factor combinations where $N = 500$ and $t_{end} - t_{start} = 6$ mean lifetimes---the combination that results in the largest sample size and hence the narrowest posterior---the average p-value of $\beta$ is still not zero, showing that even when the posterior is extremely precise it still generally contains the true parameter value. Hence, this second apparent bias is more a result of the uncertainty that we have encoded into the model, which does not align with the frequentist interpretation of uncertainty intervals that p-values measure. When $t_{start}$ is large, the upper bound of the partially observed left-truncated lifetimes is increased. Extending the support of these random variables means that the upper tails become heavier, and so too does the posterior of the Weibull shape parameter. This second form of bias is only noticeable when the number of units is very large ($N = 500$), and so, in most cases, it would be unnoticeable over the uncertainty in the posterior.

Despite both sources of bias, the $elppd$ of one hundred new observations generated from the true Weibull model is very close for all treatment-prior combinations. In Fig.~\ref{fig:sim-study-pvalue}, the size of the points indicates the average values of $elppd_{100}$ calculated according to \eqref{ep:computed_elppd} and from this we can see that the only noticeable difference between the three treatments of the left truncated data is that when there is a short period of observation,  $t_{end} - t_{start} = 0.95$, then when there are few units, $N = 10$, discarding the left-truncated observations has a large impact on the $elppd$ if analysis is not supplement with an informative joint prior. This has already been shown in Fig.~\ref{fig:joint-post-weibull}~(b) and~(e). Figure~\ref{fig:sim-study-elppd} compares the average $elppd_{100}$ scores under the different factor combinations more closely. In the figure, columns show the different levels of $t_{end} - t_{start}$ and rows show the different levels of $t_{start}$; the levels of $N$ are plotted on the horizontal axis, and the treatment of the left truncated samples and the type of prior are shown by the colour and shape of the point respectively. The plotted $elppd$ values show that when there are few units or a short period of observation, the method of imputing the partially observed values of the left-truncated samples results in an $elppd_{100}$ score that is closer to the fully observed treatment than if we were to discard the partially observed left-truncated lifetimes, except when $t_{start} = 1$ mean lifetime. The fact that it is hard to distinguish between the approaches based on the $elppd_{100}$ score, despite the bias, shows that the scale of the bias is typically inconsequential.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/ch-2/sim-results-elppd.pdf}
    \caption{The $elppd_{100}$ simulation results summarised by the mean of the one hundred repetitions for each factor combination. The columns show the different levels of $t_{end} - t_{start}$ and rows the different levels of $t_{start}$. The number of units $N$ is shown on the horizontal axis, and the average $elppd_{100}$ score is shown on the vertical axis. The colours of the points show the treatment of the left-truncated samples and the shape shows the prior.}
    \label{fig:sim-study-elppd}
\end{figure}

\section{Concluding remarks} \label{sec:weibull-conclusion}

In this chapter, I have shown that when censored observations are treated as missing and their values imputed---rather than integrated out---observations that are left truncated with unknown exposure history can be treated as a case of interval censoring and as a result, their partially observed value and truncation time imputed through Bayesian analysis. Doing so retains the information in the left truncated samples, resulting in much more precise estimates than the alternative option, which is simply to discard the left truncated lifetimes. I also develop on the method proposed by \citet{kaminskiy2005} for constructing a joint prior for the Weibull parameters by demonstrating how the choice of the two elicitation times, $t_1$ and $t_2$, dictates where in the lifetime distribution information is encoded---i.e. in the lower tail or upper tail---and implement the prior in a fully Bayesian model for left truncated and right censored lifetime data in a way so that the joint draws from the informative prior are properly filtered through the likelihood, and hence our prior belief around the CDF at both $t_1$ and $t_2$ is updated in the posterior.

I used a simulation example to demonstrate the method to impute left truncated lifetimes with unknown exposure history and their corresponding truncation times alongside the alternative case where the left truncated samples are discarded and the case where the left truncated samples are fully observed. I fit all three cases with both a vague and informative joint prior. When a vague prior is used, imputing the left truncated lifetimes with unknown exposure history retains the extra information in these samples, resulting in a much more precise posterior than when the left truncated samples are discarded and roughly the same inference as when the lifetimes are fully observed. When a carefully constructed joint prior is used, which encodes information in the upper tail of the lifetime distribution, the posterior where the left truncated samples have been discarded is much closer to the other two posteriors---where the left truncated lifetimes are fully observed or imputed.

Finally, I performed a small simulation experiment to explore the two approaches of imputing or discarding the left truncated samples with unknown exposure times---with and without an informative prior---under different simulation scenarios. The simulation experiments show that imputing the left truncated lifetimes and truncation times results in a slight bias in the parameter estimates, but in most cases this bias is small relative to the uncertainty in the posterior. There are two sources of bias. The first comes about because the treatment of observations that are both left truncated/interval censored by the start of the observation period and right censored by its end. For these lifetimes, I make the assumption that the entry time is uniformly distributed by assigning a uniform prior to the parameter $\tau^L$. However, this parameter is included in the updating procedure of the HMC algorithm. To remove the bias, the model should be re-implemented either in an alternative probabilistic programming language such as BUGs, where you can include nuisance parameters that are not updated in the MCMC procedure, or write a custom MCMC algorithm for the model. However, this is only needed if the observation period is small enough to result in a large proportion of lifetime that is left truncated and right censored.

The second source of bias arises because imputing the left truncated lifetimes and their truncation times results in uncertainty intervals that are not equivalent to frequentist credible intervals but rather encode \textit{our} uncertainty. This bias is small, and the true value of the parameters is still contained in the posterior. However, in all cases, the $elppd$ of a new set of one hundred fully observed lifetimes from the true data-generating mechanism are very close, showing that the scale of the bias is relatively negligible.

Based on the learnings from this chapter, it should be suitable to use the proposed approach for data that are left truncated with unknown installation times, so long as the observation period is sufficiently long. However, suppose there is a strong likelihood, i.e. a very large sample size, and the analyst requires a very accurate estimation of the parameter values. In such a case, it is better to discard partially observed left truncated lifetimes and, if information is available to do so, construct an informative joint prior that informs the upper tail of the distribution. When the likelihood is weaker, using the imputation method results in a posterior that is almost equivalent to the case where the installation times of the left-truncated lifetimes are known. In the next chapter, I use the methods proposed here and the findings of the simulation experiments to analyse the idler-frame dataset and show that when lifetime analysis is performed in the Bayesian framework and the censored observations are imputed, MCMC sampling naturally provides estimates and uncertainty intervals for the failure times of units still in operation, the expected number of failures in the next short time interval, and the cost per unit time of a preventative replacement strategy.